{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd6ab7b",
   "metadata": {},
   "source": [
    "# Data Analysis & Predictive Modeling Pipeline\n",
    "\n",
    "This notebook handles the complete workflow: data ingestion, exploratory analysis, predictive modeling, and output generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ddcaf",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b490f05",
   "metadata": {},
   "source": [
    "## 2. Load Data from Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths\n",
    "data_file_path = \"C:/Users/TejYadav/OneDrive - kyndryl/Data/classification_fraud.csv\"  # Change this to your file path\n",
    "output_folder = \"analysis_output\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(f\"‚úì Data loaded successfully from: {data_file_path}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nBasic info:\")\n",
    "    print(df.info())\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚úó Error: File '{data_file_path}' not found. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd881344",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc155947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(df.describe())\n",
    "\n",
    "# Missing Values Analysis\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric columns: {numeric_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Correlation Matrix\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CORRELATION MATRIX\")\n",
    "    print(\"=\" * 50)\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    print(corr_matrix)\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/01_correlation_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Correlation matrix saved\")\n",
    "\n",
    "# Distribution plots for numeric columns\n",
    "for col in numeric_cols[:6]:  # Limit to first 6 columns\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df[col].dropna(), bins=30, edgecolor='black')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(df[col].dropna())\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/02_distribution_{col}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Distribution plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eccbd6e",
   "metadata": {},
   "source": [
    "## 3.1 Target Variable & Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis (if numeric)\n",
    "if numeric_cols:\n",
    "    target_col_temp = numeric_cols[-1]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df[target_col_temp], bins=50, edgecolor='black', color='steelblue')\n",
    "    plt.xlabel(target_col_temp)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of Target Variable: {target_col_temp}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(df[target_col_temp])\n",
    "    plt.ylabel(target_col_temp)\n",
    "    plt.title(f'Boxplot of Target Variable')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/03_target_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"‚úì Target variable distribution saved\")\n",
    "\n",
    "# Categorical features analysis\n",
    "if categorical_cols:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CATEGORICAL FEATURES DISTRIBUTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for col in categorical_cols[:5]:  # Limit to first 5 categorical columns\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Count plot\n",
    "        value_counts = df[col].value_counts().head(10)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        value_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Top 10 Values in {col}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel(col)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Pie chart\n",
    "        plt.subplot(1, 2, 2)\n",
    "        top_5 = df[col].value_counts().head(5)\n",
    "        plt.pie(top_5.values, labels=top_5.index, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title(f'Top 5 Categories in {col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_folder}/03_categorical_{col}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"‚úì Categorical features analysis saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd22e99",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Handle missing values\n",
    "print(\"\\n1. Handling missing values...\")\n",
    "for col in df_processed.columns:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        if df_processed[col].dtype in [np.float64, np.int64]:\n",
    "            df_processed[col].fillna(df_processed[col].mean(), inplace=True)\n",
    "            print(f\"   - Filled {col} with mean value\")\n",
    "        else:\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "            print(f\"   - Filled {col} with mode value\")\n",
    "\n",
    "# 2. Remove duplicates\n",
    "initial_rows = len(df_processed)\n",
    "df_processed.drop_duplicates(inplace=True)\n",
    "print(f\"\\n2. Removed {initial_rows - len(df_processed)} duplicate rows\")\n",
    "\n",
    "# 3. Handle outliers using IQR method\n",
    "print(\"\\n3. Handling outliers...\")\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_processed[col].quantile(0.25)\n",
    "    Q3 = df_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df_processed[(df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"   - Found {len(outliers)} outliers in {col}\")\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete. Final shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db629a",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df_processed.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"‚úì Encoded {col}\")\n",
    "\n",
    "# Separate features and target\n",
    "# IMPORTANT: Update this based on your target variable\n",
    "target_col = numeric_cols[-1] if numeric_cols else None  # Assumes last numeric column is target\n",
    "\n",
    "if target_col is None:\n",
    "    print(\"\\n‚ö† Warning: No numeric target column found. Please specify target_col manually.\")\n",
    "else:\n",
    "    print(f\"\\nTarget variable: {target_col}\")\n",
    "    \n",
    "    X = df_processed.drop(columns=[target_col])\n",
    "    y = df_processed[target_col]\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    print(\"\\n‚úì Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0fb75",
   "metadata": {},
   "source": [
    "## 6. Build Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is None:\n",
    "    print(\"Skipping model building - target variable not defined\")\n",
    "else:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PREDICTIVE MODELING - REGRESSION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"\\nTrain set size: {X_train.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Dictionary to store models and metrics\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Linear Regression\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"1. Linear Regression\")\n",
    "    print(\"-\" * 50)\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "    \n",
    "    models['Linear Regression'] = lr_model\n",
    "    results['Linear Regression'] = {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred_lr),\n",
    "        'MSE': mean_squared_error(y_test, y_pred_lr),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_lr)),\n",
    "        'R2': r2_score(y_test, y_pred_lr)\n",
    "    }\n",
    "    \n",
    "    print(f\"MAE:  {results['Linear Regression']['MAE']:.4f}\")\n",
    "    print(f\"RMSE: {results['Linear Regression']['RMSE']:.4f}\")\n",
    "    print(f\"R¬≤:   {results['Linear Regression']['R2']:.4f}\")\n",
    "    \n",
    "    # 2. Random Forest Regressor\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"2. Random Forest Regressor\")\n",
    "    print(\"-\" * 50)\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    \n",
    "    models['Random Forest'] = rf_model\n",
    "    results['Random Forest'] = {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred_rf),\n",
    "        'MSE': mean_squared_error(y_test, y_pred_rf),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_rf)),\n",
    "        'R2': r2_score(y_test, y_pred_rf)\n",
    "    }\n",
    "    \n",
    "    print(f\"MAE:  {results['Random Forest']['MAE']:.4f}\")\n",
    "    print(f\"RMSE: {results['Random Forest']['RMSE']:.4f}\")\n",
    "    print(f\"R¬≤:   {results['Random Forest']['R2']:.4f}\")\n",
    "    \n",
    "    # Summary of all models\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(results_df)\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = results_df['R2'].idxmax()\n",
    "    print(f\"\\n‚úì Best Model: {best_model_name}\")\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    best_predictions = models[best_model_name].predict(X_test) if best_model_name == 'Random Forest' else y_pred_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22246ef",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is None:\n",
    "    print(\"Skipping evaluation - target variable not defined\")\n",
    "else:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DETAILED MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    print(\"\\nCross-Validation Scores (5-fold):\")\n",
    "    for model_name, model in models.items():\n",
    "        cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  CV R¬≤ Scores: {cv_scores}\")\n",
    "        print(f\"  Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    \n",
    "    # Feature importance for Random Forest\n",
    "    if 'Random Forest' in models:\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"Random Forest - Feature Importance\")\n",
    "        print(\"-\" * 50)\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_scaled.columns,\n",
    "            'Importance': models['Random Forest'].feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # Plot top features\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_folder}/03_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"‚úì Feature importance plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254c84d",
   "metadata": {},
   "source": [
    "## 8. Generate Insights and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60965a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is None:\n",
    "    print(\"Skipping insights - target variable not defined\")\n",
    "else:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"KEY INSIGHTS & VISUALIZATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get predictions from best model\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        y_pred_best = lr_model.predict(X_test)\n",
    "    else:\n",
    "        y_pred_best = rf_model.predict(X_test)\n",
    "    \n",
    "    # 1. Actual vs Predicted\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_test, y_pred_best, alpha=0.6)\n",
    "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel('Actual Values')\n",
    "    axes[0].set_ylabel('Predicted Values')\n",
    "    axes[0].set_title(f'Actual vs Predicted ({best_model_name})')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plot\n",
    "    residuals = y_test - y_pred_best\n",
    "    axes[1].scatter(y_pred_best, residuals, alpha=0.6)\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1].set_xlabel('Predicted Values')\n",
    "    axes[1].set_ylabel('Residuals')\n",
    "    axes[1].set_title('Residual Plot')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/04_predictions_residuals.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Predictions vs Residuals plot saved\")\n",
    "    \n",
    "    # 2. Error distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black')\n",
    "    plt.xlabel('Residual Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Prediction Errors')\n",
    "    plt.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/05_residual_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Error distribution plot saved\")\n",
    "    \n",
    "    # 3. Model comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    results_df.loc[['Linear Regression', 'Random Forest'], 'R2'].plot(kind='bar', ax=ax, color=['skyblue', 'lightcoral'])\n",
    "    ax.set_title('Model Performance Comparison (R¬≤ Score)')\n",
    "    ax.set_ylabel('R¬≤ Score')\n",
    "    ax.set_xlabel('Model')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/06_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Model comparison plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d356f1",
   "metadata": {},
   "source": [
    "## 8.1 Advanced Insights & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68370574",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is None:\n",
    "    print(\"Skipping advanced insights - target variable not defined\")\n",
    "else:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ADVANCED INSIGHTS & ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Feature Correlation with Target\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Top 15 Features Correlated with Target\")\n",
    "    print(\"-\" * 50)\n",
    "    correlations = X_scaled.copy()\n",
    "    correlations['Target'] = y.values\n",
    "    target_corr = correlations.corr()['Target'].drop('Target').abs().sort_values(ascending=False)\n",
    "    print(target_corr.head(15))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    target_corr.head(15).plot(kind='barh', color='steelblue', edgecolor='black')\n",
    "    plt.xlabel('Absolute Correlation with Target')\n",
    "    plt.title('Top 15 Features Correlated with Target Variable')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/07_target_correlations.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Target correlation plot saved\")\n",
    "    \n",
    "    # 2. Prediction Error Analysis\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Prediction Error Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    errors = np.abs(y_test.values - y_pred_best)\n",
    "    print(f\"Mean Error: {errors.mean():.4f}\")\n",
    "    print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "    print(f\"Std Dev Error: {errors.std():.4f}\")\n",
    "    print(f\"Min Error: {errors.min():.4f}\")\n",
    "    print(f\"Max Error: {errors.max():.4f}\")\n",
    "    print(f\"95th Percentile Error: {np.percentile(errors, 95):.4f}\")\n",
    "    \n",
    "    # Error distribution with statistics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Error histogram\n",
    "    axes[0, 0].hist(errors, bins=40, edgecolor='black', color='coral')\n",
    "    axes[0, 0].axvline(errors.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {errors.mean():.2f}')\n",
    "    axes[0, 0].axvline(np.median(errors), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(errors):.2f}')\n",
    "    axes[0, 0].set_xlabel('Absolute Error')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Prediction Errors')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot for residuals\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Q-Q Plot of Residuals')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction accuracy by range\n",
    "    y_range_10 = pd.cut(y_test, bins=10)\n",
    "    error_by_range = []\n",
    "    range_labels = []\n",
    "    for range_val in y_range_10.unique():\n",
    "        mask = y_range_10 == range_val\n",
    "        if mask.sum() > 0:\n",
    "            error_by_range.append(errors[mask].mean())\n",
    "            range_labels.append(f'{range_val.left:.0f}-{range_val.right:.0f}')\n",
    "    \n",
    "    axes[1, 0].plot(range_labels, error_by_range, marker='o', color='purple', linewidth=2, markersize=8)\n",
    "    axes[1, 0].set_xlabel('Target Value Range')\n",
    "    axes[1, 0].set_ylabel('Mean Error')\n",
    "    axes[1, 0].set_title('Error by Target Value Range')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative error distribution\n",
    "    sorted_errors = np.sort(errors)\n",
    "    cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100\n",
    "    axes[1, 1].plot(sorted_errors, cumulative, color='darkgreen', linewidth=2)\n",
    "    axes[1, 1].axvline(np.percentile(errors, 95), color='red', linestyle='--', label='95th percentile')\n",
    "    axes[1, 1].set_xlabel('Error Value')\n",
    "    axes[1, 1].set_ylabel('Cumulative Percentage')\n",
    "    axes[1, 1].set_title('Cumulative Distribution of Errors')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/08_error_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Detailed error analysis plot saved\")\n",
    "    \n",
    "    # 3. Model Performance Metrics Summary\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Detailed Model Performance Summary\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    from sklearn.metrics import explained_variance_score, median_absolute_error\n",
    "    \n",
    "    metrics_summary = pd.DataFrame({\n",
    "        'Linear Regression': {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred_lr),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_lr)),\n",
    "            'R¬≤': r2_score(y_test, y_pred_lr),\n",
    "            'Explained Variance': explained_variance_score(y_test, y_pred_lr),\n",
    "            'Median AE': median_absolute_error(y_test, y_pred_lr)\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred_rf),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_rf)),\n",
    "            'R¬≤': r2_score(y_test, y_pred_rf),\n",
    "            'Explained Variance': explained_variance_score(y_test, y_pred_rf),\n",
    "            'Median AE': median_absolute_error(y_test, y_pred_rf)\n",
    "        }\n",
    "    }).T\n",
    "    \n",
    "    print(metrics_summary)\n",
    "    \n",
    "    # Visualization of metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    # MAE comparison\n",
    "    metrics_summary['MAE'].plot(kind='bar', ax=axes[0], color=['skyblue', 'lightcoral'], edgecolor='black')\n",
    "    axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "    axes[0].set_ylabel('Error Value')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # RMSE comparison\n",
    "    metrics_summary['RMSE'].plot(kind='bar', ax=axes[1], color=['skyblue', 'lightcoral'], edgecolor='black')\n",
    "    axes[1].set_title('Root Mean Squared Error (RMSE)')\n",
    "    axes[1].set_ylabel('Error Value')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # R¬≤ comparison\n",
    "    metrics_summary['R¬≤'].plot(kind='bar', ax=axes[2], color=['skyblue', 'lightcoral'], edgecolor='black')\n",
    "    axes[2].set_title('R¬≤ Score (Higher is Better)')\n",
    "    axes[2].set_ylabel('R¬≤ Value')\n",
    "    axes[2].set_ylim([0, 1.1])\n",
    "    axes[2].tick_params(axis='x', rotation=0)\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/09_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Detailed metrics comparison plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951a359",
   "metadata": {},
   "source": [
    "## 9. Save Results to Output Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1950cf",
   "metadata": {},
   "source": [
    "## 8.2 Summary Insights & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc622a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is None:\n",
    "    print(\"Skipping summary - target variable not defined\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXECUTIVE SUMMARY & KEY INSIGHTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    insights = f\"\"\"\n",
    "    \n",
    "    üìä DATASET OVERVIEW\n",
    "    {'‚Äî' * 65}\n",
    "    ‚Ä¢ Total Records: {len(df):,}\n",
    "    ‚Ä¢ Features: {len(numeric_cols)} numeric, {len(categorical_cols)} categorical\n",
    "    ‚Ä¢ Total Features: {len(numeric_cols) + len(categorical_cols)}\n",
    "    ‚Ä¢ Data Quality: {(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.2f}% complete\n",
    "    \n",
    "    üìà TARGET VARIABLE: {target_col}\n",
    "    {'‚Äî' * 65}\n",
    "    ‚Ä¢ Mean: {y.mean():.4f}\n",
    "    ‚Ä¢ Median: {y.median():.4f}\n",
    "    ‚Ä¢ Std Dev: {y.std():.4f}\n",
    "    ‚Ä¢ Range: [{y.min():.4f}, {y.max():.4f}]\n",
    "    ‚Ä¢ Skewness: {y.skew():.4f}\n",
    "    \n",
    "    üéØ MODEL PERFORMANCE\n",
    "    {'‚Äî' * 65}\n",
    "    ‚Ä¢ Best Model: {best_model_name}\n",
    "    ‚Ä¢ R¬≤ Score: {r2_score(y_test, y_pred_best):.4f}\n",
    "    ‚Ä¢ MAE: {mean_absolute_error(y_test, y_pred_best):.4f}\n",
    "    ‚Ä¢ RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_best)):.4f}\n",
    "    ‚Ä¢ Mean Prediction Error: {errors.mean():.4f}\n",
    "    ‚Ä¢ Median Prediction Error: {np.median(errors):.4f}\n",
    "    \n",
    "    üîç KEY INSIGHTS\n",
    "    {'‚Äî' * 65}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Top correlated features\n",
    "    top_features = target_corr.head(3)\n",
    "    insights += \"\\n    Top Correlated Features:\\n\"\n",
    "    for i, (feat, corr) in enumerate(top_features.items(), 1):\n",
    "        insights += f\"    {i}. {feat}: {corr:.4f}\\n\"\n",
    "    \n",
    "    # Data quality insights\n",
    "    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    insights += f\"\\n    Data Quality:\\n\"\n",
    "    insights += f\"    ‚Ä¢ Missing Data: {missing_pct:.2f}%\\n\"\n",
    "    insights += f\"    ‚Ä¢ Duplicate Rows: {len(df) - len(df.drop_duplicates())}\\n\"\n",
    "    \n",
    "    # Model insights\n",
    "    lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "    rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "    \n",
    "    insights += f\"\\n    Model Comparison:\\n\"\n",
    "    insights += f\"    ‚Ä¢ Linear Regression R¬≤: {lr_r2:.4f}\\n\"\n",
    "    insights += f\"    ‚Ä¢ Random Forest R¬≤: {rf_r2:.4f}\\n\"\n",
    "    insights += f\"    ‚Ä¢ Performance Gain: {(rf_r2 - lr_r2) / lr_r2 * 100 if lr_r2 != 0 else 0:.2f}%\\n\"\n",
    "    \n",
    "    # Error distribution\n",
    "    insights += f\"\\n    Error Distribution:\\n\"\n",
    "    insights += f\"    ‚Ä¢ 50% of errors ‚â§ {np.percentile(errors, 50):.4f}\\n\"\n",
    "    insights += f\"    ‚Ä¢ 90% of errors ‚â§ {np.percentile(errors, 90):.4f}\\n\"\n",
    "    insights += f\"    ‚Ä¢ 95% of errors ‚â§ {np.percentile(errors, 95):.4f}\\n\"\n",
    "    \n",
    "    insights += f\"\\n    {'=' * 65}\\n\"\n",
    "    \n",
    "    print(insights)\n",
    "    \n",
    "    # Create a summary statistics visualization\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # Dataset information\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    ANALYSIS SUMMARY\n",
    "    Dataset Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns  |  Target: {target_col}  |  Best Model: {best_model_name}\n",
    "    \"\"\"\n",
    "    ax1.text(0.5, 0.5, summary_text, ha='center', va='center', fontsize=12, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7), family='monospace')\n",
    "    \n",
    "    # Numeric stats\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax2.axis('off')\n",
    "    numeric_stats = f\"NUMERIC FEATURES\\n{'-'*25}\\n\" + \"\\n\".join([\n",
    "        f\"{col[:15]:<15} {df[col].mean():>10.2f}\" for col in numeric_cols[:5]\n",
    "    ])\n",
    "    ax2.text(0.1, 0.5, numeric_stats, ha='left', va='center', fontsize=9, family='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "    \n",
    "    # Model metrics\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.axis('off')\n",
    "    model_stats = f\"MODEL METRICS\\n{'-'*25}\\nR¬≤ Score: {r2_score(y_test, y_pred_best):.4f}\\nMAE: {mean_absolute_error(y_test, y_pred_best):.4f}\\nRMSE: {np.sqrt(mean_squared_error(y_test, y_pred_best)):.4f}\"\n",
    "    ax3.text(0.1, 0.5, model_stats, ha='left', va='center', fontsize=9, family='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "    \n",
    "    # Error stats\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    ax4.axis('off')\n",
    "    error_stats = f\"ERROR ANALYSIS\\n{'-'*25}\\nMean: {errors.mean():.4f}\\n95th %ile: {np.percentile(errors, 95):.4f}\\nMax: {errors.max():.4f}\"\n",
    "    ax4.text(0.1, 0.5, error_stats, ha='left', va='center', fontsize=9, family='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "    \n",
    "    # Distribution plot\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    ax5.hist(y_test, bins=30, alpha=0.6, label='Actual', edgecolor='black')\n",
    "    ax5.hist(y_pred_best, bins=30, alpha=0.6, label='Predicted', edgecolor='black')\n",
    "    ax5.set_xlabel('Value')\n",
    "    ax5.set_ylabel('Frequency')\n",
    "    ax5.set_title('Target Distribution: Actual vs Predicted')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance gauge\n",
    "    ax6 = fig.add_subplot(gs[2, 2])\n",
    "    r2_score_val = r2_score(y_test, y_pred_best)\n",
    "    colors = ['red' if r2_score_val < 0.3 else 'orange' if r2_score_val < 0.6 else 'yellow' if r2_score_val < 0.8 else 'green']\n",
    "    ax6.barh([0], [r2_score_val], color=colors[0], edgecolor='black', height=0.5)\n",
    "    ax6.set_xlim([0, 1])\n",
    "    ax6.set_ylim([-0.5, 0.5])\n",
    "    ax6.set_xlabel('R¬≤ Score')\n",
    "    ax6.set_title('Model Performance')\n",
    "    ax6.set_yticks([])\n",
    "    ax6.text(r2_score_val/2, 0, f'{r2_score_val:.3f}', ha='center', va='center', \n",
    "             fontsize=14, fontweight='bold', color='white')\n",
    "    ax6.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.savefig(f\"{output_folder}/10_summary_insights.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Summary insights visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51462eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Save processed data\n",
    "processed_data_path = f\"{output_folder}/processed_data.csv\"\n",
    "df_processed.to_csv(processed_data_path, index=False)\n",
    "print(f\"‚úì Processed data saved to: {processed_data_path}\")\n",
    "\n",
    "# 2. Save predictions\n",
    "if target_col is not None:\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual': y_test.values,\n",
    "        'Predicted': y_pred_best,\n",
    "        'Residual': y_test.values - y_pred_best\n",
    "    })\n",
    "    predictions_path = f\"{output_folder}/predictions.csv\"\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "    print(f\"‚úì Predictions saved to: {predictions_path}\")\n",
    "\n",
    "# 3. Save model metrics report\n",
    "metrics_report = {\n",
    "    'Analysis_Date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'Dataset_Shape': df.shape,\n",
    "    'Processed_Shape': df_processed.shape,\n",
    "    'Target_Variable': str(target_col),\n",
    "    'Best_Model': best_model_name,\n",
    "    'Model_Metrics': results\n",
    "}\n",
    "\n",
    "report_path = f\"{output_folder}/model_metrics_report.json\"\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(metrics_report, f, indent=4, default=str)\n",
    "print(f\"‚úì Model metrics report saved to: {report_path}\")\n",
    "\n",
    "# 4. Save feature list\n",
    "feature_list_path = f\"{output_folder}/features.txt\"\n",
    "with open(feature_list_path, 'w') as f:\n",
    "    f.write(\"FEATURE LIST\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Total Features: {len(X_scaled.columns)}\\n\\n\")\n",
    "    f.write(\"Features:\\n\")\n",
    "    for i, feature in enumerate(X_scaled.columns, 1):\n",
    "        f.write(f\"{i}. {feature}\\n\")\n",
    "    if target_col:\n",
    "        f.write(f\"\\n\\nTarget Variable: {target_col}\\n\")\n",
    "print(f\"‚úì Feature list saved to: {feature_list_path}\")\n",
    "\n",
    "# 5. Save insights summary\n",
    "summary_path = f\"{output_folder}/analysis_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"DATA ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Original Data Shape: {df.shape}\\n\")\n",
    "    f.write(f\"Processed Data Shape: {df_processed.shape}\\n\")\n",
    "    f.write(f\"Target Variable: {target_col}\\n\\n\")\n",
    "    \n",
    "    if target_col is not None:\n",
    "        f.write(\"\\nMODEL PERFORMANCE\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for model_name, metrics in results.items():\n",
    "            f.write(f\"\\n{model_name}:\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                f.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n\\nBest Model: {best_model_name}\\n\")\n",
    "\n",
    "print(f\"‚úì Analysis summary saved to: {summary_path}\")\n",
    "\n",
    "# 6. List all saved files\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"OUTPUT FILES GENERATED\")\n",
    "print(\"=\" * 50)\n",
    "saved_files = os.listdir(output_folder)\n",
    "for i, file in enumerate(saved_files, 1):\n",
    "    file_path = os.path.join(output_folder, file)\n",
    "    file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "    print(f\"{i}. {file} ({file_size:.2f} KB)\")\n",
    "\n",
    "print(f\"\\n‚úì All results saved to folder: {output_folder}/\")\n",
    "print(\"‚úì Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
